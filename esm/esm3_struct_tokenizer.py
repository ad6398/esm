# -*- coding: utf-8 -*-
"""ESM3-struct-tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18yIBkXlAzkGgOZcF-74aO9G_NqgK5AkB

## instaLL
"""

!pip install git+https://github.com/ad6398/esm.git

! pip install biotite
# ! pip install py3Dmol
# ! pip install matplotlib
# ! pip install dna-features-viewer

from biotite.database import rcsb
from esm.sdk.api import ESMProtein
from esm.utils.structure.protein_chain import ProteinChain
from esm.utils.types import FunctionAnnotation

VQVAE_CODEBOOK_SIZE = 4096
VQVAE_SPECIAL_TOKENS = {
    "MASK": VQVAE_CODEBOOK_SIZE,
    "EOS": VQVAE_CODEBOOK_SIZE + 1,
    "BOS": VQVAE_CODEBOOK_SIZE + 2,
    "PAD": VQVAE_CODEBOOK_SIZE + 3,
    "CHAINBREAK": VQVAE_CODEBOOK_SIZE + 4,
}

"""## loss

"""

import torch

def backbone_distance_loss(
    X_pred: torch.Tensor,
    X_true: torch.Tensor
) -> torch.Tensor:
    """
    Compute the 'backbone_distance_loss' in batch:
      - X_pred, X_true: (B, L, 3, 3)
      - B = batch size
      - L = number of backbone transforms/residues
      - each item is a 3x3 matrix

    Steps (Algorithm 10):
      1) Flatten each 3x3 matrix to length 9 => (B, L, 9)
      2) For each batch element, compute pairwise squared distances
         => D_pred, D_true of shape (B, L, L)
      3) Take the squared difference (D_pred - D_true)^2
      4) Clamp at 25
      5) Mean over all entries
    """
    B, L, _, _ = X_pred.shape

    # 1) Flatten: from (B, L, 3, 3) -> (B, L, 9)
    Xp = X_pred.view(B, L, 9)
    Xt = X_true.view(B, L, 9)

    # 2) Pairwise squared distances within each batch
    #    xp_diff has shape (B, L, L, 9) => sum along last dim => (B, L, L)
    xp_diff = Xp.unsqueeze(2) - Xp.unsqueeze(1)
    D_pred  = xp_diff.pow(2).sum(dim=-1)  # shape = (B, L, L)

    xt_diff = Xt.unsqueeze(2) - Xt.unsqueeze(1)
    D_true  = xt_diff.pow(2).sum(dim=-1)  # shape = (B, L, L)

    # 3) Elementwise squared difference
    E = (D_pred - D_true).pow(2)

    # 4) Clamp at 25
    E = torch.clamp(E, max=25.0)

    # 5) Mean over all batch entries and pairwise indices
    return E.mean()



def compute_vectors(X: torch.Tensor) -> torch.Tensor:
    """
    Given backbone coordinates X of shape (B, L, 3, 3),
    where each residue has [N, CA, C] in the 3rd dimension,
    returns the 6 direction vectors for each residue:

      (B, L, 6, 3)

    Vectors:
      v1 = (N -> CA)
      v2 = (CA -> C)
      v3 = (C -> N_next)
      v4 = - (v1 x v2)
      v5 = (C_prev->N) x (v1)
      v6 = (v2) x (v3)
    """
    # X shape: (B, L, 3, 3)
    # Each residue: N = X[...,0,:], CA = X[...,1,:], C = X[...,2,:]

    N  = X[:, :, 0, :]  # (B, L, 3)
    CA = X[:, :, 1, :]  # (B, L, 3)
    C  = X[:, :, 2, :]  # (B, L, 3)

    # v1 = CA - N
    v1 = CA - N
    # v2 = C - CA
    v2 = C - CA

    # We will zero-pad for next_N if there's no next residue
    next_N = torch.zeros_like(N)
    # for all residues except the last, the next_N is N of (i+1)
    next_N[:, :-1, :] = N[:, 1:, :]

    # v3 = (C -> N_next)
    v3 = next_N - C

    # v4 = - (v1 x v2)
    v4 = - torch.cross(v1, v2, dim=-1)

    # For the previous residueâ€™s C (zero-padded at start):
    prev_C = torch.zeros_like(C)
    prev_C[:, 1:, :] = C[:, :-1, :]

    # v0 = (C_prev -> N)
    v0 = N - prev_C

    # v5 = (C_prev->N) x (v1)
    v5 = torch.cross(v0, v1, dim=-1)

    # v6 = (v2) x (v3) = (CA->C) x (C->N_next)
    v6 = torch.cross(v2, v3, dim=-1)

    # Stack to (B, L, 6, 3)
    return torch.stack([v1, v2, v3, v4, v5, v6], dim=2)


def backbone_direction_loss(X_pred: torch.Tensor, X_true: torch.Tensor) -> torch.Tensor:
    """
    X_pred, X_true: (B, L, 3, 3), each with (N,CA,C) coords for L residues.

    Implements the "backbone_direction_loss" by:
      1) computing 6 direction vectors for each residue
      2) computing all-pairs dot products
      3) taking squared difference, clamp at 20, mean
    """
    # 1. Compute the 6 vectors per residue
    V_pred = compute_vectors(X_pred)  # (B, L, 6, 3)
    V_true = compute_vectors(X_true)  # (B, L, 6, 3)

    # 2. Flatten from (B, L, 6, 3) to (B, 6L, 3)
    B, L = V_pred.shape[0], V_pred.shape[1]
    V_pred = V_pred.view(B, L*6, 3)
    V_true = V_true.view(B, L*6, 3)

    # 3. Pairwise dot-products in each batch
    # D_pred[i,j] = dot(V_pred[i], V_pred[j])
    # => shape (B, 6L, 6L)
    D_pred = torch.matmul(V_pred, V_pred.transpose(-1, -2))
    D_true = torch.matmul(V_true, V_true.transpose(-1, -2))

    # 4. Squared difference, clamp at 20
    E = (D_pred - D_true).pow(2)
    E = torch.clamp(E, max=20.0)

    # 5. Mean over all batch entries and pairwise indices
    return E.mean()

"""## model

"""

import torch
import torch.nn as nn
from esm.models.vqvae import (
    StructureTokenDecoder,
    StructureTokenEncoder,
)

class StructureTokenVQVAE(nn.Module):
    def __init__(
        self,
        structure_encoder,
        structure_decoder,
        freeze_encoder=False
    ):
        super().__init__()

        self.encoder = structure_encoder
        self.decoder = structure_decoder
        self.freeze_encoder = freeze_encoder
        self.encoder.codebook.freeze_codebook = False

        if self.freeze_encoder:
            self._freeze_encoder()

    def _freeze_encoder(self):
        """Freeze encoder parameters including the codebook."""
        for param in self.encoder.parameters():
            param.requires_grad = False

    def forward(
        self,
        coords: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        sequence_id: torch.Tensor | None = None,
        residue_index: torch.Tensor | None = None,
        structure_tokens: torch.Tensor | None = None
    ):
        """
        Forward function that can perform encoding and decoding.
        If `structure_tokens` is provided, it runs decoding. Otherwise, it encodes and decodes.
        """
        coords =  coords[..., :3, :]

        if structure_tokens is None:
            # Encoding Step
            z_q, min_encoding_indices = self.encoder.encode(
                coords, attention_mask, sequence_id, residue_index
            )
            structure_tokens = min_encoding_indices

            batch_size = structure_tokens.size(0)

            # 1. Replace first token with BOS token ID
            structure_tokens[:, 0] = VQVAE_SPECIAL_TOKENS["BOS"]

            # 2. Replace last token where attention_mask is 1 with EOS token ID
            eos_positions = attention_mask.sum(dim=1) - 1  # Get the last position where attention_mask is 1
            batch_indices = torch.arange(batch_size, device=structure_tokens.device)
            structure_tokens[batch_indices, eos_positions] = VQVAE_SPECIAL_TOKENS["EOS"]

            # 3. Replace tokens with PAD token where attention_mask is 0
            structure_tokens = torch.where(attention_mask == 0,
                                         VQVAE_SPECIAL_TOKENS["PAD"],
                                         structure_tokens)


        # Decoding Step
        # print shape of all tensors
        # print(f"coords shape: {coords.shape}")
        # print(f"structure_tokens shape: {structure_tokens.shape}")
        # print(f"attention_mask shape: {attention_mask.shape}")
        # if sequence_id:
        #     print(f"sequence_id shape: {sequence_id.shape}")
        outputs = self.decoder.decode(structure_tokens, attention_mask, sequence_id)
        # for k in outputs:
        #     print(k, outputs[k].shape)

        x_pred = outputs['bb_pred']
        dist_loss = backbone_distance_loss(x_pred, coords)
        dir_loss = backbone_distance_loss(x_pred, coords)
        # print(f"dist_loss: {dist_loss}, dir_loss: {dir_loss}")

        loss = dist_loss + dir_loss

        return {"loss": loss, "output": outputs}

structure_encoder = StructureTokenEncoder(d_model=256, n_heads=1, v_heads=128, n_layers=2, d_out=128, n_codes=4096)
structure_decoder = StructureTokenDecoder(d_model=512, n_heads=8, n_layers=8)

model = StructureTokenVQVAE(structure_encoder=structure_encoder, structure_decoder=structure_decoder)

"""## dataset"""



from esm.utils.structure.protein_chain import ProteinChain
from esm.sdk.api import ESMProtein
from torch.utils.data import DataLoader, Dataset

# from transformers import DataCollator

class ProteinDataCollator:
    def __call__(self, batch):
        max_length = 512

        padded_coords = []
        padded_sequences = []
        attention_masks = []

        # print("batch", batch)
        for item in batch:
            seq_length = len(item["coords"])
            if seq_length > max_length - 2:  # -2 to account for BOS and EOS tokens
                seq_length = max_length - 2
                item["coords"] = item["coords"][:seq_length,:,:]

            pad_length = max_length - seq_length - 2  # -2 for BOS and EOS

            # Add BOS, original coords, EOS, and padding
            padded_coord = torch.cat([
                torch.zeros((1, 37, 3)),  # BOS token
                item["coords"],
                torch.zeros((1, 37, 3)),  # EOS token
                torch.zeros((pad_length, 37, 3))  # padding
            ], dim=0)

            # Attention mask: 1 for BOS, sequence, and EOS; 0 for padding
            attention_mask = [1] + [1] * seq_length + [1] + [0] * pad_length

            padded_coords.append(padded_coord)
            attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))

        return {
            "coords": torch.stack(padded_coords),
            # "sequence": torch.stack(padded_sequences),
            "attention_mask": torch.stack(attention_masks),
        }

# count = 0
class SingleChainProteinDataset(Dataset):
    def __init__(self, pdb_ids):
        self.pdb_ids = pdb_ids
        self.chain_id = 'detect'

    def __len__(self):
        return len(self.pdb_ids)

    def __getitem__(self, idx):
        pdb_id = self.pdb_ids[idx]

        # str_io = rcsb.fetch(pdb_id, "pdb")
        # print(f"idx {idx}, pdb_id: {pdb_id}")
        # protein_chain = ProteinChain.from_rcsb(pdb_id)
        str_io = rcsb.fetch(pdb_id, "pdb")
        protein_chain = ProteinChain.from_pdb(str_io, chain_id = self.chain_id, id=pdb_id)

        protein = ESMProtein.from_protein_chain(protein_chain)
        coords = protein.coordinates

        # # Check first 3 atoms (N, CA, C) of each residue
        backbone_atoms = coords[:, :3, :]  # shape: (seq_len, 3, 3)

        # Check for NaN values
        if torch.isnan(backbone_atoms).any():
            raise ValueError(f"NaN values found in backbone atoms for PDB {pdb_id}, chain_id {protein_chain.chain_id}, idx {idx}")
            # count += 1

        # Check for zero vectors (all components = 0)
        zero_mask = (backbone_atoms == 0).all(dim=-1)  # shape: (seq_len, 3)
        if zero_mask.any():
            raise ValueError(f"Zero coordinates found in backbone atoms for PDB {pdb_id}")

        return {"coords": coords}

def get_pdbids_from_file(file_path):
    pdb_list = []
    with open(file_path, 'r') as fp:

        for line in fp.readlines():
            text = line
            pdb_list.extend(text.split(","))

    return pdb_list

import random

def split_into_train_valid_test(pdb_ids_list, valid_frac=0.1, test_frac=0.1, seed=42):
    """
    Splits a list of PDB IDs into training, validation, and test sets.

    Parameters:
        pdb_ids_list (list): List of PDB IDs.
        valid_frac (float): Fraction of data to allocate to validation set.
        test_frac (float): Fraction of data to allocate to test set.
        seed (int): Random seed for reproducibility.

    Returns:
        tuple: (train_set, valid_set, test_set)
    """
    assert 0 <= valid_frac < 1 and 0 <= test_frac < 1, "Fractions must be between 0 and 1"
    assert valid_frac + test_frac < 1, "Sum of valid_frac and test_frac must be less than 1"

    # Set random seed for reproducibility
    random.seed(seed)

    # Shuffle the list
    pdb_ids_list = list(pdb_ids_list)  # Ensure it's a list in case of other iterables
    random.shuffle(pdb_ids_list)

    # Compute split indices
    total_size = len(pdb_ids_list)
    valid_size = int(total_size * valid_frac)
    test_size = int(total_size * test_frac)

    # Split data
    test_set = pdb_ids_list[:test_size]
    valid_set = pdb_ids_list[test_size:test_size + valid_size]
    train_set = pdb_ids_list[test_size + valid_size:]

    return train_set, valid_set, test_set

pdb_list_files = ['/content/validate_pdb_ids_poc.txt' ]

all_pdb_ids = []
for f in pdb_list_files:
    all_pdb_ids.extend(get_pdbids_from_file(f))

train_pdb_ids, valid_pdb_ids, test_pdb_ids = split_into_train_valid_test(all_pdb_ids)

train_dataset = SingleChainProteinDataset(pdb_ids=train_pdb_ids)
valid_dataset = SingleChainProteinDataset(pdb_ids=valid_pdb_ids)
test_dataset = SingleChainProteinDataset(pdb_ids=test_pdb_ids)

train_dataset.__len__()

"""## trainer"""

from transformers import Trainer, TrainingArguments

# Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    eval_steps= 50,
    # eval_strategy= 'steps',
    # save_strategy="steps",
    logging_dir="./logs",
    logging_steps=10,
    report_to='none'
)

model = model.to("cuda")

data_collator = ProteinDataCollator()
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    tokenizer=None, # No traditional tokenizer; we process structures directly
    data_collator= data_collator
)

trainer.train()