# -*- coding: utf-8 -*-
"""ESM3-struct-tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18yIBkXlAzkGgOZcF-74aO9G_NqgK5AkB

## instaLL
"""

!pip install git+https://github.com/ad6398/esm.git

! pip install biotite
# ! pip install py3Dmol
# ! pip install matplotlib
# ! pip install dna-features-viewer

from biotite.database import rcsb
from esm.sdk.api import ESMProtein
from esm.utils.structure.protein_chain import ProteinChain
from esm.utils.types import FunctionAnnotation

VQVAE_CODEBOOK_SIZE = 4096
VQVAE_SPECIAL_TOKENS = {
    "MASK": VQVAE_CODEBOOK_SIZE,
    "EOS": VQVAE_CODEBOOK_SIZE + 1,
    "BOS": VQVAE_CODEBOOK_SIZE + 2,
    "PAD": VQVAE_CODEBOOK_SIZE + 3,
    "CHAINBREAK": VQVAE_CODEBOOK_SIZE + 4,
}

"""## model

"""

import torch
import torch.nn as nn
from esm.models.vqvae import (
    StructureTokenDecoder,
    StructureTokenEncoder,
)

class StructureTokenVQVAE(nn.Module):
    def __init__(
        self,
        structure_encoder,
        structure_decoder,
        freeze_encoder=False
    ):
        super().__init__()

        self.encoder = structure_encoder
        self.decoder = structure_decoder
        self.freeze_encoder = freeze_encoder
        self.encoder.codebook.freeze_codebook = False

        if self.freeze_encoder:
            self._freeze_encoder()

    def _freeze_encoder(self):
        """Freeze encoder parameters including the codebook."""
        for param in self.encoder.parameters():
            param.requires_grad = False

    def forward(
        self,
        coords: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        sequence_id: torch.Tensor | None = None,
        residue_index: torch.Tensor | None = None,
        structure_tokens: torch.Tensor | None = None
    ):
        """
        Forward function that can perform encoding and decoding.
        If `structure_tokens` is provided, it runs decoding. Otherwise, it encodes and decodes.
        """

        if structure_tokens is None:
            # Encoding Step
            z_q, min_encoding_indices = self.encoder.encode(
                coords, attention_mask, sequence_id, residue_index
            )
            structure_tokens = min_encoding_indices

            batch_size = structure_tokens.size(0)

            # 1. Replace first token with BOS token ID
            structure_tokens[:, 0] = VQVAE_SPECIAL_TOKENS["BOS"]

            # 2. Replace last token where attention_mask is 1 with EOS token ID
            eos_positions = attention_mask.sum(dim=1) - 1  # Get the last position where attention_mask is 1
            batch_indices = torch.arange(batch_size, device=structure_tokens.device)
            structure_tokens[batch_indices, eos_positions] = VQVAE_SPECIAL_TOKENS["EOS"]

            # 3. Replace tokens with PAD token where attention_mask is 0
            structure_tokens = torch.where(attention_mask == 0,
                                         VQVAE_SPECIAL_TOKENS["PAD"],
                                         structure_tokens)


        # Decoding Step
        # print shape of all tensors
        print(f"structure_tokens shape: {structure_tokens.shape}")
        print(f"attention_mask shape: {attention_mask.shape}")
        if sequence_id:
            print(f"sequence_id shape: {sequence_id.shape}")
        outputs = self.decoder.decode(structure_tokens, attention_mask, sequence_id)
        return outputs

structure_encoder = StructureTokenEncoder(d_model=1024, n_heads=1, v_heads=128, n_layers=2, d_out=128, n_codes=4096)
structure_decoder = StructureTokenDecoder(d_model=1280, n_heads=20, n_layers=30)

model = StructureTokenVQVAE(structure_encoder=structure_encoder, structure_decoder=structure_decoder)

"""## dataset"""



from esm.utils.structure.protein_chain import ProteinChain
from esm.sdk.api import ESMProtein
from torch.utils.data import DataLoader, Dataset

# from transformers import DataCollator

class ProteinDataCollator:
    def __call__(self, batch):
        max_length = 510

        padded_coords = []
        padded_sequences = []
        attention_masks = []

        # print("batch", batch)
        for item in batch:
            seq_length = len(item["coords"])
            if seq_length > max_length - 2:  # -2 to account for BOS and EOS tokens
                seq_length = max_length - 2
                item["coords"] = item["coords"][:seq_length,:,:]

            pad_length = max_length - seq_length - 2  # -2 for BOS and EOS

            # Add BOS, original coords, EOS, and padding
            padded_coord = torch.cat([
                torch.zeros((1, 37, 3)),  # BOS token
                item["coords"],
                torch.zeros((1, 37, 3)),  # EOS token
                torch.zeros((pad_length, 37, 3))  # padding
            ], dim=0)

            # Attention mask: 1 for BOS, sequence, and EOS; 0 for padding
            attention_mask = [1] + [1] * seq_length + [1] + [0] * pad_length

            padded_coords.append(padded_coord)
            attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))

        return {
            "coords": torch.stack(padded_coords),
            # "sequence": torch.stack(padded_sequences),
            "attention_mask": torch.stack(attention_masks),
        }

class SingleChainProteinDataset(Dataset):
    def __init__(self, pdb_ids):
        self.pdb_ids = pdb_ids
        self.chain_id = 'A'

    def __len__(self):
        return len(self.pdb_ids)

    def __getitem__(self, idx):
        pdb_id = self.pdb_ids[idx]

        str_io = rcsb.fetch(pdb_id, "pdb")
        print(f"idx {idx}, pdb_id: {pdb_id}")
        protein_chain = ProteinChain.from_pdb(str_io, chain_id=self.chain_id, id=pdb_id)
        protein = ESMProtein.from_protein_chain(protein_chain)

        return {"coords": protein.coordinates}

def get_pdbids_from_file(file_path):
    pdb_list = []
    with open(file_path, 'r') as fp:

        for line in fp.readlines():
            text = line
            pdb_list.extend(text.split(","))

    return pdb_list

import random

def split_into_train_valid_test(pdb_ids_list, valid_frac=0.1, test_frac=0.1, seed=42):
    """
    Splits a list of PDB IDs into training, validation, and test sets.

    Parameters:
        pdb_ids_list (list): List of PDB IDs.
        valid_frac (float): Fraction of data to allocate to validation set.
        test_frac (float): Fraction of data to allocate to test set.
        seed (int): Random seed for reproducibility.

    Returns:
        tuple: (train_set, valid_set, test_set)
    """
    assert 0 <= valid_frac < 1 and 0 <= test_frac < 1, "Fractions must be between 0 and 1"
    assert valid_frac + test_frac < 1, "Sum of valid_frac and test_frac must be less than 1"

    # Set random seed for reproducibility
    random.seed(seed)

    # Shuffle the list
    pdb_ids_list = list(pdb_ids_list)  # Ensure it's a list in case of other iterables
    random.shuffle(pdb_ids_list)

    # Compute split indices
    total_size = len(pdb_ids_list)
    valid_size = int(total_size * valid_frac)
    test_size = int(total_size * test_frac)

    # Split data
    test_set = pdb_ids_list[:test_size]
    valid_set = pdb_ids_list[test_size:test_size + valid_size]
    train_set = pdb_ids_list[test_size + valid_size:]

    return train_set, valid_set, test_set

pdb_list_files = ['/content/rcsb_pdb_ids_c98f35e40b774ba5e9e180a7486f6c7e_00001-10000.txt' ]

all_pdb_ids = []
for f in pdb_list_files:
    all_pdb_ids.extend(get_pdbids_from_file(f))

train_pdb_ids, valid_pdb_ids, test_pdb_ids = split_into_train_valid_test(all_pdb_ids)

train_dataset = SingleChainProteinDataset(pdb_ids=train_pdb_ids)
valid_dataset = SingleChainProteinDataset(pdb_ids=valid_pdb_ids)
test_dataset = SingleChainProteinDataset(pdb_ids=test_pdb_ids)

"""## trainer"""

from transformers import Trainer, TrainingArguments

# Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    report_to='none'
)

model = model.to("cuda")

data_collator = ProteinDataCollator()
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=None, # No traditional tokenizer; we process structures directly
    data_collator= data_collator
)

trainer.train()

